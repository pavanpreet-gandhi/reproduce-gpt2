{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. Simple training loop\n",
    "1. Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2 import GPT, GPTConfig # our GPT class\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding('gpt2')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # dynamic device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(GPTConfig).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "    \"\"\"\n",
    "    A simple data loader for a text file.\n",
    "    \"\"\"\n",
    "    def __init__(self, B: int, T: int, device: torch.device):\n",
    "        with open('data/input.txt', 'r') as file:\n",
    "            text = file.read().replace('\\n', '')\n",
    "        self.B, self.T = B, T\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long, device=device)\n",
    "        self.current_batch = 0\n",
    "        self.number_of_batches = len(self.tokens) // (B * T)\n",
    "        print(f'Loaded {len(self.tokens)} tokens, {self.number_of_batches} batches of size {B}x{T}')\n",
    "\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_batch * B * T : (self.current_batch + 1) * B * T + 1]\n",
    "        x = buf[:-1].view(B, T)\n",
    "        y = buf[1:].view(B, T)\n",
    "        self.current_batch += 1\n",
    "        if self.current_batch >= self.number_of_batches:\n",
    "            self.current_batch = 0\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 297884 tokens, 2327 batches of size 4x32\n",
      "Loss at iteration 0: 11.060534477233887\n",
      "Loss at iteration 1: 10.472539901733398\n",
      "Loss at iteration 2: 9.853291511535645\n",
      "Loss at iteration 3: 9.531839370727539\n",
      "Loss at iteration 4: 9.502036094665527\n",
      "Loss at iteration 5: 9.306251525878906\n",
      "Loss at iteration 6: 9.232611656188965\n",
      "Loss at iteration 7: 8.98526382446289\n",
      "Loss at iteration 8: 8.579424858093262\n",
      "Loss at iteration 9: 9.132532119750977\n",
      "Loss at iteration 10: 8.336834907531738\n",
      "Loss at iteration 11: 8.353912353515625\n",
      "Loss at iteration 12: 8.109197616577148\n",
      "Loss at iteration 13: 8.144638061523438\n",
      "Loss at iteration 14: 8.191824913024902\n",
      "Loss at iteration 15: 8.942989349365234\n",
      "Loss at iteration 16: 7.978180885314941\n",
      "Loss at iteration 17: 8.345073699951172\n",
      "Loss at iteration 18: 8.077600479125977\n",
      "Loss at iteration 19: 7.648392677307129\n",
      "Loss at iteration 20: 7.580885887145996\n",
      "Loss at iteration 21: 7.497943878173828\n",
      "Loss at iteration 22: 7.643979549407959\n",
      "Loss at iteration 23: 7.986662864685059\n",
      "Loss at iteration 24: 8.104787826538086\n",
      "Loss at iteration 25: 7.377313613891602\n",
      "Loss at iteration 26: 7.625522613525391\n",
      "Loss at iteration 27: 7.856736183166504\n",
      "Loss at iteration 28: 7.514963626861572\n",
      "Loss at iteration 29: 8.6121244430542\n",
      "Loss at iteration 30: 8.18187427520752\n",
      "Loss at iteration 31: 8.633744239807129\n",
      "Loss at iteration 32: 8.179093360900879\n",
      "Loss at iteration 33: 8.86424446105957\n",
      "Loss at iteration 34: 8.295170783996582\n",
      "Loss at iteration 35: 8.1596040725708\n",
      "Loss at iteration 36: 8.0574369430542\n",
      "Loss at iteration 37: 8.008538246154785\n",
      "Loss at iteration 38: 7.417298316955566\n",
      "Loss at iteration 39: 7.608996868133545\n",
      "Loss at iteration 40: 7.463644504547119\n",
      "Loss at iteration 41: 7.1231513023376465\n",
      "Loss at iteration 42: 7.343317985534668\n",
      "Loss at iteration 43: 7.395937442779541\n",
      "Loss at iteration 44: 7.731119632720947\n",
      "Loss at iteration 45: 7.056443691253662\n",
      "Loss at iteration 46: 7.656207084655762\n",
      "Loss at iteration 47: 7.495243072509766\n",
      "Loss at iteration 48: 6.986733913421631\n",
      "Loss at iteration 49: 7.668717861175537\n",
      "Loss at iteration 50: 7.41004753112793\n",
      "Loss at iteration 51: 7.194637298583984\n",
      "Loss at iteration 52: 6.757769584655762\n",
      "Loss at iteration 53: 7.426570415496826\n",
      "Loss at iteration 54: 6.900676727294922\n",
      "Loss at iteration 55: 6.853733539581299\n",
      "Loss at iteration 56: 7.288853645324707\n",
      "Loss at iteration 57: 7.056180000305176\n",
      "Loss at iteration 58: 7.0348711013793945\n",
      "Loss at iteration 59: 6.947371482849121\n",
      "Loss at iteration 60: 7.017998218536377\n",
      "Loss at iteration 61: 7.6699299812316895\n",
      "Loss at iteration 62: 7.041904449462891\n",
      "Loss at iteration 63: 7.051379680633545\n",
      "Loss at iteration 64: 7.386332988739014\n",
      "Loss at iteration 65: 7.581233978271484\n",
      "Loss at iteration 66: 7.09343957901001\n",
      "Loss at iteration 67: 6.664775371551514\n",
      "Loss at iteration 68: 7.142937183380127\n",
      "Loss at iteration 69: 6.930028438568115\n",
      "Loss at iteration 70: 6.673150539398193\n",
      "Loss at iteration 71: 7.5875091552734375\n",
      "Loss at iteration 72: 6.988966464996338\n",
      "Loss at iteration 73: 6.742754936218262\n",
      "Loss at iteration 74: 6.725167751312256\n",
      "Loss at iteration 75: 6.918404579162598\n",
      "Loss at iteration 76: 7.592223644256592\n",
      "Loss at iteration 77: 8.155632972717285\n",
      "Loss at iteration 78: 7.651791095733643\n",
      "Loss at iteration 79: 7.931485652923584\n",
      "Loss at iteration 80: 7.466166019439697\n",
      "Loss at iteration 81: 7.6353759765625\n",
      "Loss at iteration 82: 6.954606533050537\n",
      "Loss at iteration 83: 7.263321876525879\n",
      "Loss at iteration 84: 6.742138862609863\n",
      "Loss at iteration 85: 6.90597677230835\n",
      "Loss at iteration 86: 7.361388683319092\n",
      "Loss at iteration 87: 7.230716705322266\n",
      "Loss at iteration 88: 6.529590606689453\n",
      "Loss at iteration 89: 6.982753753662109\n",
      "Loss at iteration 90: 6.7605743408203125\n",
      "Loss at iteration 91: 6.995656490325928\n",
      "Loss at iteration 92: 8.28003978729248\n",
      "Loss at iteration 93: 8.337523460388184\n",
      "Loss at iteration 94: 7.160513877868652\n",
      "Loss at iteration 95: 6.704478740692139\n",
      "Loss at iteration 96: 7.577248573303223\n",
      "Loss at iteration 97: 7.391157150268555\n",
      "Loss at iteration 98: 6.69713830947876\n",
      "Loss at iteration 99: 7.45609712600708\n"
     ]
    }
   ],
   "source": [
    "B, T = 4, 32\n",
    "data_loader = DataLoaderLite(B, T, device)\n",
    "\n",
    "model.train();\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(100):\n",
    "    x, y = data_loader.next_batch()\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Loss at iteration {i}: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial x: torch.Size([3, 5])\n",
      "Generated x: torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "batches = 3\n",
    "k = 10\n",
    "num_generations = 5\n",
    "\n",
    "prompt = \"Hello, my name is\"\n",
    "tokens = enc.encode(prompt)\n",
    "x = torch.tensor(tokens).to(device) # (T)\n",
    "x = x.unsqueeze(0).repeat(batches, 1) # (B, T)\n",
    "print('Initial x:', x.shape)\n",
    "\n",
    "model.eval();\n",
    "for i in range(num_generations): # generate num_generation tokens\n",
    "    with torch.no_grad(): # no need to track gradients\n",
    "        logits, _ = model(x) # (B, T, Vocab)\n",
    "        logits = logits[:, -1, :] # (B, Vocab)\n",
    "        probs = F.softmax(logits, dim=-1) # (B, Vocab)\n",
    "        topk_probs, topk_x = torch.topk(probs, k) # (B, k), (B, k)\n",
    "        ix = torch.multinomial(topk_probs, num_samples=1) # (B, 1)\n",
    "        next_x = topk_x.gather(dim=1, index=ix) # (B, 1)\n",
    "        x = torch.cat((x, next_x), dim=-1) # (B, T+1)\n",
    "print('Generated x:', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Hello, my name is. himI:I\n",
      "Batch 2: Hello, my name isUS:I,,\n",
      "Batch 3: Hello, my name isUS: and,US\n"
     ]
    }
   ],
   "source": [
    "for i in range(batches):\n",
    "    print(f'Batch {i+1}:', enc.decode(x[i].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
